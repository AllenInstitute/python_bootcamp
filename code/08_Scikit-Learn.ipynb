{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e63e881-1900-4b9d-8f93-aca2e2a0d45a",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<img src=\"support_files/images/cropped-SummerWorkshop_Header.png\">  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2617f6-9beb-4a28-8067-eb97bf6c407f",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<center><h1>Hands-On scikit-learn Tutorial: Getting Started with Machine Learning</h1><br>\n",
        "    <img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/01/scikit-learn-logo.png\">\n",
        "</center>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e5e146-8a8a-49db-a38a-34eb6a2cc86c",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<h2>Introduction</h2>\n",
        "\n",
        "<p>Scikit-learn, also known as sklearn, is a popular and widely-used open-source machine learning library in Python. This library provides a robust set of tools for various machine learning tasks, including supervised and unsupervised learning which are a central focus of this tutorial. Scikit-learn is built on top of other scientific computing libraries like NumPy and SciPy, making it a powerful and efficient choice for data scientists and machine learning practitioners.\n",
        "\n",
        "<p>The primary goal of scikit-learn is to simplify the process of applying machine learning algorithms to real-world data. It offers a user-friendly interface for working with data, creating machine learning models, and evaluating their performance. The library supports a broad range of algorithms, from traditional statistical methods to cutting-edge machine learning techniques, making it suitable for both beginners and experienced researchers.\n",
        "\n",
        "<p>The purpose of this tutorial is to provide a high-level overview of supervised and unsupervised learning and describe how to utilize scikit-learn to build machine learning models tailored for these specific tasks. By the end of this tutorial, you will have a solid understanding of the key concepts, methods, and techniques required to effectively leverage scikit-learn for creating powerful and accurate machine learning models in both supervised and unsupervised settings. \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6424a39-1396-4306-b388-ed9720468cb1",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<h2>Supervised Learning</h2>\n",
        "\n",
        "<p>Supervised learning is a machine learning approach where an algorithm learns from labeled training data to make accurate predictions or decisions. In this setting, a dataset consists of input-output pairs, where each input is associated with a corresponding desired output. The goal is to learn a model (or function) that maps inputs to outputs (or targets) from the training set so that the model can then accurately predict outputs for new, unseen inputs.\n",
        "\n",
        "\n",
        "<p>Based on this description there are 3 main ingredients to supervised learning: (1) inputs, (2) targets, and (3) model. Let's consider an example of learning a model that predicts house prices.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4a2c7b-c463-4956-864d-0cb80f182f6b",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "1. **Inputs**\n",
        "\n",
        "First, we are given a set of _inputs_ which we denote as $X=\\{x^{(1)},\\ldots, x^{(n)}\\}$. An input $x^{(i)}$ is a vector where each entry is a feature that describes that data point. Using the housing price prediction example, some relevant features for a house are the number of bedrooms and square footage. If we have a new house with 4 bedrooms and 2500 square footage, then the feature vector for this house is $x^{(i)}=$ (4, 2500).\n",
        "\n",
        "\n",
        "2. **Targets**\n",
        "\n",
        "Second, we are given a set of _targets_ denoted as $y=\\{y^{(1)},\\ldots, y^{(n)}\\}$ such that each target $y^{(i)}$ is the value that the model is trying to predict or estimate based on the input $x^{(i)}$. Targets represent the values we are aiming to understand, predict, or classify using the trained machine learning model. Using the housing price prediction example, the label would be the actual price of the house corresponding to the input data. For example, suppose that our new house with 3 bedrooms and 1800 square footage is sold for $\\$500,000$, then the target would be $y^{(i)}=500,000$.\n",
        "\n",
        "3. **Model**\n",
        "\n",
        "Lastly, our objective is to train a _model_ on a dataset that contains examples of inputs along with corresponding desired label. The model learns from these examples by identifying underlying relationships and adjusting its internal parameters to optimize its performance. In this example, our goal is to learn some function $f$ such that $f(x^{(i)})\\approx y^{(i)}$, then use this function to predict the value of a house that is about to be listed. \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5e0574-8bfe-48b7-b8f4-87446b9aa9b1",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h3>Let's Explore a Real Dataset!</h3>\n",
        "\n",
        "<p>The California Housing Prices dataset is a well-known and frequently used dataset in the field of machine learning and housing market analysis. It provides information about housing characteristics and pricing across various regions in California, making it valuable for regression and predictive modeling tasks. The California Housing Prices dataset contains features that describe different aspects of housing neighborhoods in California, such as median income, average housing occupancy, median house age, and more. The target variable is the median house value for California districts. Researchers and data analysts commonly use the California Housing Prices dataset to explore and analyze the relationships between housing attributes and prices.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2904053-719e-4f46-baf9-443adbd4127e",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "**Key Information**:\n",
        "    \n",
        "(https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)\n",
        "\n",
        "Number of training examples: 20,640<br>\n",
        "Target: Median house value (in $100k)<br>\n",
        "Number of Features: 8 numerical features<br>\n",
        "Feature Information:\n",
        "\n",
        "- MedInc:        median income in block group\n",
        "- HouseAge:      median house age in block group\n",
        "- AveRooms:      average number of rooms per household\n",
        "- AveBedrms:     average number of bedrooms per household\n",
        "- Population:    block group population\n",
        "- AveOccup:      average number of household members\n",
        "- Latitude:      block group latitude\n",
        "- Longitude:     block group longitude\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65a4abf-2956-4132-8a47-4de5d8b66158",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<h4>Let's load the data!</h4>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9488d566-1655-426c-a3db-af5b5d7da320",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing_dataset = fetch_california_housing(as_frame=True)\n",
        "housing_dataset.frame.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db87a40-8d74-4e75-927c-4e23cd13e2bd",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h4>Now let's visualize the data!</h4>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eca7abb-3492-410a-a18f-2d826c9989d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_dataset.frame.hist(figsize=(10, 7), bins=30, edgecolor=\"black\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14eef67-af0a-4bb1-96d0-e4e8e1d1737c",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h4>Are there Patterns in this Data?</h4>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5462d44-628a-4b28-a04f-a6fb1871e500",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fix, ax = plt.subplots()\n",
        "sns.scatterplot(\n",
        "    data=housing_dataset.frame,\n",
        "    x=\"Longitude\",\n",
        "    y=\"Latitude\",\n",
        "    size=\"MedHouseVal\",\n",
        "    hue=\"MedHouseVal\",\n",
        "    palette=\"viridis\",\n",
        "    alpha=0.5,\n",
        "    ax=ax,\n",
        ")\n",
        "ax.set_aspect('equal')\n",
        "plt.legend(title=\"MedHouseVal\", bbox_to_anchor=(1.05, 0.95), loc=\"upper left\")\n",
        "_ = plt.title(\"Median house value per district,\\ndepending on spatial location\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a6ea248-1cc7-4cb1-8b70-9f1ac0378048",
      "metadata": {},
      "source": [
        "<div class=\"exercise\" style=\"background: #DFF0D8; border-radius: 3px; padding: 10px; color: #000;\">\n",
        "\n",
        "<h4>Exercise: Is any single feature predictive of the target MedHouseVal?</h4>\n",
        "\n",
        "Try selecting different features to plot here and see which are most predictive of house value.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321106b3-6e59-4419-81aa-5c7edd878edb",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6e43148f-5aa9-413c-bc08-68473a977d45",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h4>Simple Model: Linear Regression</h4>\n",
        "\n",
        "<p>Linear regression is a method to find a straight line that best fits data points. It helps predict a target value based on one or more input variables. The goal is to minimize the difference between predicted and actual values. This technique is used for tasks like predicting prices, understanding relationships, and identifying trends in data. We're going to start by splitting the data into train and test sets. The train set is the portion of the dataset used to teach a model by showing it input data and expected outcomes. The test set is a separate part of the data used to evaluate the model's performance by comparing its predictions with the actual outcomes it hasn't seen before.\n",
        "\n",
        "<p><center><img src=\"https://i0.wp.com/thaddeus-segura.com/wp-content/uploads/2020/09/3.1.1.1.1-Linear-Regression.png?resize=1024%2C498&ssl=1\"></center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4d8d70-0826-4294-977d-ffb492e168d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = housing_dataset.data\n",
        "y = housing_dataset.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"Number of training examples:\", len(X_train))\n",
        "print(\"Number of testing examples:\", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af5db46-9618-458f-939b-c78960acc1f5",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h4>1. Choose a class of model</h4>\n",
        "\n",
        "In Scikit-Learn, every class of model is represented by a Python class.\n",
        "So, for example, if we would like to compute a simple `LinearRegression` model, we can import the linear regression class:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "669ca161-ebd7-4317-8d25-f7204a5be7df",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3641494f-941f-4d18-98eb-59a922fe7b31",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "Note that other more general linear regression models exist as well; you can read more about them in the [`sklearn.linear_model` module documentation](http://Scikit-Learn.org/stable/modules/linear_model.html).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0167772c-b1ea-453e-9310-d8131da71e16",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h4>2. Choose model hyperparameters</h4>\n",
        "\n",
        "An important point is that *a class of model is not the same as an instance of a model*.\n",
        "\n",
        "Once we have decided on our model class, there are still some options open to us.\n",
        "Depending on the model class we are working with, we might need to answer one or more questions like the following:\n",
        "\n",
        "- Would we like to fit for the offset (i.e., *y*-intercept)?\n",
        "- Would we like the model to be normalized?\n",
        "- Would we like to preprocess our features to add model flexibility?\n",
        "- What degree of regularization would we like to use in our model?\n",
        "- How many model components would we like to use?\n",
        "\n",
        "These are examples of the important choices that must be made *once the model class is selected*.\n",
        "These choices are often represented as *hyperparameters*, or parameters that must be set before the model is fit to data.\n",
        "In Scikit-Learn, hyperparameters are chosen by passing values at model instantiation.\n",
        "We will explore how you can quantitatively choose hyperparameters in [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb).\n",
        "\n",
        "For our linear regression example, we can instantiate the `LinearRegression` class and specify that we would like to fit the intercept using the `fit_intercept` hyperparameter:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee86cc4e-aabd-4642-b42d-b6dd8e626e63",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LinearRegression(fit_intercept=True)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9022ea79-ae4e-4556-84e3-687df22cb243",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "Keep in mind that when the model is instantiated, the only action is storing the hyperparameter values.\n",
        "In particular, we have not yet applied the model to any data: the Scikit-Learn API makes very clear the distinction between *choice of model* and *application of model to data*.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae4e9f4-ab16-43a0-983d-9dfcdd2909d6",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h4>4. Fit the model to the data</h4>\n",
        "\n",
        "Now it is time to apply our model to the data.\n",
        "This can be done with the `fit` method of the model. \n",
        "    \n",
        "First, let's try fitting just against median income per district:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e83c48-8dbc-49bb-a7b1-ba766a45462a",
      "metadata": {},
      "outputs": [],
      "source": [
        "features_to_fit = ['MedInc']\n",
        "model.fit(X_train[features_to_fit], y_train)\n",
        "print(f\"Model fit coefficient: {model.coef_[0]}  intercept: {model.intercept_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3970ad99-3e7e-4f37-97a6-120dac43f3c0",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "This fit command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore. In Scikit-Learn, by convention all model parameters that were learned during the fit process have trailing underscores; for example in this linear model, the `coef_` and `intercept_` attributes printed above determine the line of best fit.\n",
        "<br><br>\n",
        "Let's see how this line matches the data we fit the model against:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8ebbfc-50e9-4ff7-93f4-747a8a5c4ae8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Scatter plot of training data\n",
        "x_train = X_train['MedInc']\n",
        "ax.scatter(x_train, y_train, alpha=0.1)\n",
        "\n",
        "# Line showing best fit\n",
        "xlim = np.array([x_train.min(), x_train.max()])\n",
        "ylim = xlim * model.coef_[0] + model.intercept_\n",
        "ax.plot(xlim, ylim, color='red');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57b8cc6b-d4df-4e6e-b6cd-c2632be4d41c",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h4>5. Predict target values for unknown data</h4>\n",
        "\n",
        "<p>Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set. In Scikit-Learn, this can be done using the <code>predict()</code> method:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8e87a24-1f86-41c2-835d-ade67c54ad3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# how accurate is this model?\n",
        "import sklearn.metrics\n",
        "y_predicted = model.predict(X_test[features_to_fit])\n",
        "print(f\"mean squared error: {sklearn.metrics.mean_squared_error(y_predicted, y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f029aa2a-51b5-4ede-a6d2-a3215e32758d",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "What if instead we fit a hyperplane against all 8 features; do we get better prediction performance?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7fe616-2695-4be7-a703-33f1edb28bb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the model again, this time using all 8 input features\n",
        "model.fit(X_train, y_train)\n",
        "print(f\"Model fit coefficients: \\n{model.coef_}  \\nintercept: {model.intercept_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4225d9e-6942-4e3a-aa07-8cdf39adeaa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_predicted = model.predict(X_test)\n",
        "print(f\"mean squared error: {sklearn.metrics.mean_squared_error(y_predicted, y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba8752f-54f5-4c3a-b96e-14c90745acab",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "Another way we can explore the performance of the model is to plot target versus predicted house values:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617c4a2a-5607-4ab3-9bd4-8bfbac9a5ab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_test, y_predicted, alpha=0.2)\n",
        "ax.set_aspect('equal')\n",
        "ax.plot([0, 5], [0, 5], color=(0.3, 0.3, 0.3), linewidth=1)\n",
        "ax.set_ylim(-0.5, 5.5)\n",
        "ax.set_xlim(-0.5, 5.5)\n",
        "ax.set_xlabel('Target house value')\n",
        "ax.set_ylabel('Predicted house value')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "328bb24d-f05e-4e2f-af1a-6326eb34b053",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "The plot above gives us some clues about when we can expect this model to fail and how we might design a better model.<br><br>\n",
        "    \n",
        "One question that frequently comes up regards the uncertainty in such internal model parameters.\n",
        "In general, Scikit-Learn does not provide tools to draw conclusions from internal model parameters themselves: interpreting model parameters is much more a *statistical modeling* question than a *machine learning* question.\n",
        "Machine learning instead focuses on what the model *predicts*.\n",
        "\n",
        "If you would like to dive into the meaning of fit parameters within the model, other tools are available, including the <a href=\"http://statsmodels.sourceforge.net/\">`statsmodels` Python package</a>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5fa8641-59ac-482a-90e0-c192a9cadfec",
      "metadata": {
        "tags": []
      },
      "source": [
        "<div class=\"exercise\" style=\"background: #DFF0D8; border-radius: 3px; padding: 10px; color: #000;\">\n",
        "\n",
        "<b>Exercise: See if you can achieve higher regression accuracy by using a more complex model.</b>\n",
        "\n",
        "* Select a model from https://scikit-learn.org/stable/supervised_learning.html\n",
        "* Create an instance of the model. What hyperparameters do you need to think about here?\n",
        "* Fit the model against X_train, y_train\n",
        "* Use the model to generate a prediction from X_test and compare to y_test\n",
        "* Does your model perform better or worse than the linear regression? Why?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01334b1-b521-4838-89c8-6f71001034fe",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "15e1461f-eccd-42c1-82f6-16b631711806",
      "metadata": {
        "tags": []
      },
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<h2>Unsupervised Learning</h2>\n",
        "\n",
        "<p>In <i>supervised</i> learning, we begin with a set of known inputs and outputs (targets), then generate a model that is capable of mapping from inputs to outputs. <i>Unsupervised</i> learning is similar, but instead we begin with only the inputs, and the job of the model is to come up with a set of meaningful outputs that best summarize the inputs using less data. \n",
        "\n",
        "<h3>Data dimensionality</h3>\n",
        "\n",
        "<p>The California housing dataset introduced above contains 20,640 houses with 8 different numerical features per house.\n",
        "We can think of each house as a vector of length 8 that specifies a point inside an 8-dimensional space.\n",
        "Note that this is different from the `X_train.ndim`! The training data are stored in a 2D table, but the *dimensionality* of this dataset is higher.\n",
        "    \n",
        "<p>8-dimensional spaces are difficult to visualize and reason about. Fortunately, the points in this dataset occupy a fairly restricted subset of this 8-D space. It may be possible, then, to describe the most noteworthy features of the dataset using fewer dimensions, if we are willing to discard aspects of the data that seem less noteworthy. \n",
        "\n",
        "<p>The goal of unsupervised learning is to automatically discover such simpler descriptions. It should be clear that there is usually no correct way to do this--how we reduce the dimensionality of our data, and which aspects of the data to keep or discard, is a matter of art. Treat the results of unsupervised learning as a hypothesis to be tested!\n",
        "\n",
        "<p>Further reading: <a href=\"https://scikit-learn.org/stable/unsupervised_learning.html\">https://scikit-learn.org/stable/unsupervised_learning.html</a>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d58809b-2f1e-4f21-b8fd-6f9fac015777",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<h3>Let's start with a fake dataset</h3>\n",
        "\n",
        "<p>This dataset will consist of 1000 samples, each described by 20 features (a 1000x20 array). \n",
        "\n",
        "<p>We could imagine this is, for example, a description of 1000 neurons, where each neuron is desctibed by features like \"average firing rate\", \"width of cell body\", \"number of input synapses\", etc. \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86cdb4bc-a028-423e-a01f-56ad2dcee585",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask sklearn to make a fake dataset for us:\n",
        "import sklearn.datasets\n",
        "data, target = sklearn.datasets.make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_classes=4, \n",
        "    n_clusters_per_class=1,\n",
        "    class_sep=1.9,\n",
        "    n_informative=10,\n",
        "    random_state=4,\n",
        ")\n",
        "print(\"Data shape:\", data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9abd30a9-7419-46ae-9a62-a5bf861d80da",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<p>Each neuron is described by a vector of length 20, and thus the space of all possible descriptions is 20-dimensional. We suspect that the neurons do not uniformly fill this space, that there is some structure to the data -- for example, some features may be correlated with each other, or perhaps the neurons form distinct clusters within this space. In other words, some significant part of this space is <i>empty</i>, and therefore we don't need the full 20 degrees of freedom to adequately summarize our data.\n",
        "\n",
        "<p>To begin to visualize the data, let's just pick pairs of features and plot these against each other:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdc2caa-4df1-4914-a8a4-3aed7ff3607b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's just look at the relationship between pairs of features (since flat screens are good at visualizing 2 dimensions at a time).\n",
        "# This is a kind of very simple dimensionality reduction in which we pick two dimensions to keep, and discard the rest.\n",
        "import matplotlib.pyplot as plt\n",
        "n_rows, n_cols = 5, 5\n",
        "fig, ax = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
        "color = plt.cm.Set1(target)\n",
        "for row in range(n_rows):\n",
        "    ax[row, 0].set_ylabel(f\"feature {row+1}\")\n",
        "    for col in range(n_cols):\n",
        "        ax[-1, col].set_xlabel(f\"feature {col+1}\")\n",
        "        if row == col:\n",
        "            ax[row, col].hist(data[:, row], bins=30)\n",
        "        else:\n",
        "            ax[row, col].scatter(data[:, row], data[:, col], s=2,\n",
        "                # color=color,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d68c038-d2cb-4923-b5c8-359d72f239ac",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<p>When we look at 2 dimensions at a time, we see some structure: correlations between features tells us that the features are not totally independent of one another.\n",
        "    \n",
        "<p>But maybe we could see more structure if we were able to combine more than 2 dimensions at a time? \n",
        "\n",
        "<p>Here we'll use scikit-learn to do a principal component analysis (PCA) to determine such an optimal combination of features:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ac6a3a-b1f8-4921-bd0e-6c1fa157780f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scikit-learn follows the same pattern here that we used for supervised learning:\n",
        "\n",
        "# Create a model instance\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the model to our data\n",
        "pca.fit(data)\n",
        "\n",
        "# Transform the data\n",
        "pca_reduced = pca.transform(data)\n",
        "\n",
        "# The result has 1000 rows just like the input data, but now only 2 features per row.\n",
        "pca_reduced.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f226faf-dcfd-4364-8877-6981874609b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(pca_reduced[:, 0], pca_reduced[:, 1], \n",
        "    # color=color\n",
        ")\n",
        "ax.set_xlabel('PCA component 1')\n",
        "ax.set_ylabel('PCA component 2');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67183d3c-016e-4380-8470-6b05b3b2021e",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "\n",
        "<p>Scikit-learn has many models that provide different approaches to dimensionality reduction. \n",
        "\n",
        "<p>A more powerful (and less interpretable) method is UMAP, which is actually not included in sklearn, but nonetheless implements a compatible API following the same pattern:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d47932-08d8-4fa4-a97a-c5d94d7fed52",
      "metadata": {},
      "outputs": [],
      "source": [
        "from umap import UMAP\n",
        "\n",
        "# Create a model instance\n",
        "umap = UMAP(\n",
        "    n_components=2,\n",
        "    n_neighbors=20,\n",
        "    min_dist=0.4,\n",
        ")\n",
        "\n",
        "# Fit the model to data\n",
        "umap.fit(data)\n",
        "\n",
        "# Transform the data\n",
        "umap_reduced = umap.transform(data)\n",
        "umap_reduced.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7478e39f-772e-488b-af50-ace9614daade",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(umap_reduced[:, 0], umap_reduced[:, 1], \n",
        "    # color=color\n",
        ")\n",
        "ax.set_xlabel('UMAP Feature 1')\n",
        "ax.set_ylabel('UMAP Feature 2');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d3898c-721f-40b9-9b2d-16f47a21705d",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<p>At this point it looks clear that there is an even simpler way to describe our dataset:\n",
        "\n",
        "<p>Rather than each neuron being represented as a 2D vector, we could instead assign each a single integer label for each of the 4 apparent clusters.\n",
        "\n",
        "<p>To do this, we use another unsupervised learning method called clustering.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a02eec0-39a8-4518-bcb4-30874cfbc975",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<h3>Clustering</h3>\n",
        "\n",
        "The dimensionality reduction methods we looked at above were able to reduce our 20-dimensional dataset down to 2D, and plotting these reduced vectors revealed some hidden structure in the data. Now we will use K-means clustering to further reduce the 2D data down to a 1D label.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583669a6-4594-4464-9fe3-f638c319e319",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a model instance\n",
        "kmeans = KMeans(n_clusters=4, n_init='auto', random_state=42)\n",
        "\n",
        "# fit the model to our UMAP result\n",
        "kmeans.fit(umap_reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a4940e-551b-4566-9204-04b083056f0a",
      "metadata": {},
      "source": [
        "<div class=\"default\" style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; color: #000;\">\n",
        "<h4>Visualize the Clustering </h4>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43448a9c-e03a-4530-8e5c-4d7497a4bda9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coordinates of cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "# Cluster assignments for each data point\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Plot clusters\n",
        "plt.scatter(umap_reduced[:, 0], umap_reduced[:, 1], c=labels, s=50, cmap='viridis', alpha=0.8)\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X')\n",
        "plt.title(\"K-Means Clustering Results\")\n",
        "plt.xlabel(\"UMAP Feature 1\")\n",
        "plt.ylabel(\"UMAP Feature 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d860c8-a616-4970-9d72-236b9f949ac5",
      "metadata": {
        "tags": []
      },
      "source": [
        "<div class=\"exercise\" style=\"background: #DFF0D8; border-radius: 3px; padding: 10px; color: #000;\">\n",
        "\n",
        "<b>Exercise: Play with the UMAP inputs and parameters to see ways you might end up with a different result.</b>\n",
        "\n",
        "See: https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
        "    \n",
        "* How is the result affected by having less data to fit (try 300 or 30 samples)\n",
        "* Try low (2) and high (200) values of n_neighbors\n",
        "* Try low (0.05) and high (1.0) values of min_dist\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "pytorch-gpu.1-13.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}